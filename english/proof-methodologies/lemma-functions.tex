Assertions provide a way to give clues to the verification condition generator
so the SMT solvers will get enough enough information to make the proofs we
need. However, it is sometimes hard to write exactly the assertion that will
create exactly the property needed by the SMT solver to trigger the right lemma
(for example, since the generator makes some optimization on the goal that
might slightly modify it). Furthermore, we rely on lemmas that often need to be
proved with the Coq proof assistant, and that means that we need to learn Coq.



In this section, we will see some techniques that we can use to make all of this
more predictible and does not require from us to use the Coq proof assistant.
While these techniques cannot be used in any case (and we will explain what are
the cases when it is not applicable), they are quite efficient to get almost
full automatic proof. This relies on ghost code.



\levelThreeTitle{Proof by induction}



Previously, we mentioned that SMT solvers are bad at reasoning by induction
(most of them), and this is the reason why we often need to express lemmas that
we then prove using the Coq proof assistant that allows us to write our proof
by induction. However, in the section~\ref{l2:statements-loops} about loops, we
find a subsection~\ref{l3:statements-loops-invariant} named ``Induction and
invariant'' where we explain how to prove that a loop does the right job ... by
induction. What is this sorcery?!




In fact, it is quite simple. When we prove a loop invariant by induction using
SMT solvers, they do not have to perform the reasoning by induction themselves.
The job of splitting the proof into two subproofs, one for the establishment of
the invariant (the base case of the proof), and one for the preservation (the
induction case) is performed by the verification condition generator. So when
the verification conditions are transmitted to the SMT solvers, this work is not
needed anymore.




How can we exploit this idea? We explained before that ghost code can be used to
provide more information than what is explicitely provided by the source code.
For this, we add some code (and possibly annotations about this code) that
allows to deduce more properties. Let us illustrate this with a simple example.
In a previous exercise (\ref{l4:acsl-properties-lemmas-lsorted-gsorted}), we
wanted to prove the following function call (we have excluded the postconditions
to shorten the example):



\CodeBlockInput{c}{ghost-code-usage-1.c}



For this, the solution that was asked in the exercise was to provide a lemma
that states that if a range is ``locally sorted'', meaning that each element
is greater or equals to the one that precedes it, then we can say that it is
``globally sorted'', that is to say for each pair of indices $i$ and $j$, if
$i \leq j$ then the $j^{th}$ of the array is greater or equals to the $i^{th}$
element. Then, the precondition could be proved by SMT solvers, but not the
lemma itself that requires a Coq proof. Can we do something using a loop?



The answer is yes. Basically, we build a proof that, because we know that the
array is locally sorted, we can deduce that it is globally sorted, before
calling the function (which is basically a proof of the lemma we would need).
We want to prove that the range is globally sorted, to write this proof by hand,
we would procede by induction on the size of the range. We have two cases.
First, the range is empty and the property trivially true. Now, let us suppose
that some range of size $i$ with $i < length$ ($length$ being the size of the
complete range), is globally sorted and show that if it is the case, then the
range of size $i+1$ is sorted. This is easy because, by our precondition, we
know that the $i^{th}$ element is greater than the $(i-1)^{th}$ element, that is
itself greater than all the preceding elements.




Now, how can we translate this into ghost code? We write a loop that goes from
$0$ (our base case), to the end of the range \CodeInline{len} and provide as an
invariant that the array is globally sorted from $0$ to the current step of the
loop. We also add some assertions to help the provers (namely the fact that the
current element is greater than the one that precedes it):



\CodeBlockInput[15][31]{c}{ghost-code-usage-2.c}



And we can see that all verification conditions are easily verified by SMT
solvers, without requiring to write a Coq proof of lemma. This kind of code is
called a proof carrying code: we have written some code and annotations that
carries a proof that some property we want to verify holds. However, we had to
write the ghost code directly in annotation of the program, and that mean that
if we have another call somewhere else in the code with some similar
precondition, we would have to do it again. Let us make this easier by using
lemma functions.



\levelThreeTitle{Lemma function}



The principle of lemma functions is basically the same as lemmas: from some
premises, we want to prove some conclusion. And once it has been done, we want
to use it at some other place to directly deduce the conclusion from the
premises without having to do the proof again, by instancing it with actual
values.



The way to do this is to use a function, using the \CodeInline{requires} to
express the premisses of the lemma, and the \CodeInline{ensures} to express the
conclusion of the lemma. The universally quantified variables can either still
be quantified, or correspond to a parameter of the function. Namely, if a
variable is only bounded to premises, or only to conlusions, it can be
translated into a universally quantified variable, if it is bounded to both
premises and conclusion, it must be a parameter of a function (as we cannot
quantify a variable for all a function contract in ACSL).



Let us first consider an example where we do not use (directly) universally
quantified variables in the contract, with our previous example about sorted
values. From the property \CodeInline{element\_level\_sorted(arr, len)}, we
want to deduce \CodeInline{sorted(arr, len)}. The corresponding lemma could be:


\begin{CodeBlock}{c}
/*@
  lemma element_level_sorted_is_sorted:
    \forall int* arr, integer len ;
       element_level_sorted(arr, len) ==> sorted(arr, len) ;
*/
\end{CodeBlock}


So let us write a function that takes two parameters: \CodeInline{arr} and
\CodeInline{len}, requires that the array is locally sorted and ensures that it
is globally sorted:



\CodeBlockInput[16][21]{c}{lemma-function-1-1.c}



Note that this function must assign \CodeInline{\textbackslash{}nothing}, indeed
we use it to deduce some properties about the program, with ghost code, and
thus it should not modify the content of the array, else the ghost code would
modify the behavior of the program. Now let us provide a body to this function,
the proof carrying code that the conclusion is true when the precondition is true.
It corresponds to the code we previously wrote to prove the precondition of
the call to \CodeInline{bsearch}:



\CodeBlockInput[16][31]{c}{lemma-function-1-2.c}



With this specified loop, with we get an inductive proof that the lemma holds,
now we can use this property by simply calling the lemma function when we need
to perform this deduction:



\CodeBlockInput[34][40]{c}{lemma-function-1-2.c}



Which asks us to establish the premises thanks to the precondition of the lemma
function (and which we trivially get from the precondition of the
\CodeInline{bsearch\_callee} function), and provides us the conclusion for free
thanks to the postcondition of the lemma function (and we can use it as a
precondition for the call to \CodeInline{bsearch}).



As we explained, when universally quantified variable are bounded to both
conclusion and premises, they must be parameters, and it is the case here for
the variables \CodeInline{arr} and \CodeInline{len}. Whereas the quantified
variable that are used in the predicates:



\CodeBlockInput[4][8]{c}{lemma-function-1-2.c}



as they are only bound to respectvely the premise and the conclusion remain
universally quantified (even if it is hidden by the predicate). We could for
example have written the contract like this:



\CodeBlockInput[16][31]{c}{lemma-function-1-3.c}



where we perfectly see that variables are still universally quantified. However,
we are not forced to maintain them universally quantified, and we could
perfectly translate them into parameters (provided that the conclusion we want
to get from the premises still makes sense). Let us for example translate the
\CodeInline{i} and \CodeInline{j} of the conclusion into parameters:



\CodeBlockInput[34][41]{c}{lemma-function-1-3.c}



Which is also perfectly fine and we could for example use this function to
deduce some properties about the content of the array. Note that here, we use a
call to the previous lemma function to make the proof easier. We even can go
further by transfering the ``premise of our conclusion'' as another premise of
a new lemma:



\CodeBlockInput[44][52]{c}{lemma-function-1-3.c}



All these lemmas state the same global relation, the difference is related to
the amount of information that is required to instanciate them (and thus the
precision of the property that we get in return).



Finally, let us present a last usage of lemma functions. On all previsous
examples, we have considered only universally quantified variable. In fact, what
we have said before is applicable to existentially quantified variables: if they
are bound to both premises and conclusions, they must be parameters, else they
can either be parameters or remain quantified. However, about existentially
quantified variables, we sometimes can go further by building a function that
directly provide a witness for an existentially quantified formula.



For example, let us consider the axiomatic definition for occurrence counting,
and imagine that at some point in a program, we want to prove the following
assertion from the precondition:



\CodeBlockInput[24][32]{c}{lemma-function-2-1.c}



Of course, there exists some index \CodeInline{n} such that \CodeInline{in[n]}
is \CodeInline{v}, else the number of occurrences of this value would be $0$.
But, instead of just proving that such an index exists, let us directly find
some index respect the constraints on \CodeInline{n} by using a lemma function
that returns it:



\CodeBlockInput[24][42]{c}{lemma-function-2-2.c}



If we only look at the body of the function, it has two behaviors: either some
cell of the array contains \CodeInline{v} and the function returns its index, or
there is not, and then the function returns -1. The first behavior is easy to
show, the return is performed in a branch where we know that the considered
index corresponds to a cell that is in the range of the array and has a value
\CodeInline{v}.



We prove that the second behavior respects the postcondition by showing that it
leads to a contradiction. If there is not a cell of value \CodeInline{v}, then
the number of occurrences of \CodeInline{v} is 0, this is expressed by the
second invariant that show that as we have not met any \CodeInline{v} since we
started, the number of occurrences is 0. However, the precondition of the
function states that the number of occurrences is more than $0$ which leads to
a contradiction that we model model by an assertion of false (note that this is
not necessary, we explicitely write it for our explanation).



Finally, we can call this function to show that there exists some index that
allows our assertion to be validated:



\CodeBlockInput[44][53]{c}{lemma-function-2-2.c}



The use of lemma functions makes reasoning by induction feasible for lemmas
without the need of interactive proof. Furthermore, the triggerring of lemmas
becomes more predictible as we instanciate them by hand. However, while lemmas
can consider multiple labels:



\begin{CodeBlock}{c}
/*@
  lemma my_lemma{L1, L2}:  P{L1} ==> P{L2} ;
*/
\end{CodeBlock}



Lemma functions do not provide an equivalent mechanism as they are basically
normal C functions that cannot take labels in input. Let us show what we can
do when we need such a construct.



\levelThreeTitle{Lemma macro}



When we have to deal with multiple labels, the idea is to directly ``inject''
the proof carrying code at the place where it is needed exactly as we did at the
beginning of the section. However, we do not want to write this code by hand
everytime we need such a proof, so let us use macros to do it.



For now, let us translate our previous code into a macro instead of a function.
As we use this macro in ghost code (thus, in annotation) we have to take care to
use the ghost annotation syntax to write the invariant of the loop and the
assertions:



\CodeBlockInput[16][33]{c}{lemma-macro-1.c}



Instead of providing a pre and a postcondition, we state these properties using
assertions before and after the proof carrying code. The proof carrying code
itslef is basically the same as before, and it is used exactly as it was used in
the case of functions. However, we can see that it makes an important difference
once it has been preprocessed by Frama-C as the block of code and annotations is
directly injected in the function \CodeInline{bsearch\_callee}.



\image{lemma-macro-1}



So in fact, we use the macro to generate the code we previously wrote. In this
case, it is not really interesting as a function call allows us to make things
more modular. So let us study a case where we do not have any other choice than
using a macro.



We illustrate using the following lemma:



\CodeBlockInput[4][11]{c}{lemma-macro-2-1.c}


In order to prove the following program:


\CodeBlockInput[13][29]{c}{lemma-macro-2-1.c}


Where the lemma \CodeInline{shift\_ptr} is necessary to prove the postcondition
of \CodeInline{callee} from the postcondition of \CodeInline{shift\_array}. Our
goal is of course to get rid of the lemma, replacing it by a lemma macro.



There is no precise guideline for designing a macro used from the injection of
proof carrying code. However, most lemmas stated about multiple labels are quite
similar in the way they related labels. So let us illustrate with this example,
most of the time designing a macro in such a situation will more or less follow
the same scheme.



In order to build the macro, we need a context where we can work on it. We build
the context using a function, let us name this function
\CodeInline{context\_to\_prove\_shift\_ptr}. The idea is to use the function to
build the macro in isolation of the rest of the program to make the verification
of the property easier. However, while lemma functions are then called to deduce
some properties in some other function, this function will never be called, its
only role is to provide us a ``place'' where we can build our proof. In
particular, as we need multiple memory labels, our function \textbf{needs} to
modify the content of the memory (else, there is a single memory state for all
the function).



Let us illustrate with our current problem to make all of this clearer. First,
we create a macro \CodeInline{shift\_array} that will contain our proof carrying
code, for now let us just indicate that it is an empty statement. In the
parameters of this lemma, we take the labels that are considered. Note that the
rules we previously mentioned about quantified variables still apply to macros.


\begin{CodeBlock}{c}
#define shift_ptr(_L1, _L2, _arr, _fst, _last, _s1, _s2) ;
\end{CodeBlock}


Then we create our context function:


\CodeBlockInput[22][41]{c}{lemma-macro-2-2.c}


Let us decompose this code, starting from the context function. In input, we
recieve all the variables of the lemma. We also state some properties about the
bounds of the integer values we consider, basically these should be requirements
that are not related to memory states. Then, we introduce the label
\CodeInline{L1} and we call the function \CodeInline{assign\_array} that leads
us to the label \CodeInline{L2}. The role of this call is to ensure that WP will
create a new memory label (thus, it will not consider that the memory is the
same), and to establish our premises. Indeed, if we have a look to the contract
of \CodeInline{assign\_array}, we see that it assigns the array (which
guarantees the creation of a new memory label) and in postcondition, it ensures
that the content of the array, between the pre and the postcondition (thus, when
we call it: \CodeInline{L1} and \CodeInline{L2}) respects the premise of our
lemma (which we repeat on line 36, by adding an assertion). Then we use our
\CodeInline{shift\_ptr} macro (that will later contain the proof carrying code),
and we then expect to be able to prove the postcondition of our lemma (line 40).



By doing this, we ensure that we built a context that only contains the
information we need to build the proof carrying code that allows us to deduce
the conclusion (line 40) from the premise (line 36). Now let us write the macro.



\CodeBlockInput[9][19]{c}{lemma-macro-2-2.c}



We will not detail this code which is quite similar to what we have written in
the beginning of this section. the only small subtlety is the assert that helps
the SMT solvers to relate the memory locations between \CodeInline{L1} and
\CodeInline{L2} together. Finally, we can complete the proof of our function by
using our lemma:



\CodeBlockInput[52][61]{c}{lemma-macro-2-2.c}



As one could notice, while this techniques allows to inject the proof carrying
code with a single line of code, it can inject quite a lot of code and
annotations everytime we use it. Thus, it can make proof context bigger, and
harder to use for SMT solvers. There are other limitations to this technique and
the careful reader might have notice them. Let us now talk about it.



\levelThreeTitle{Limitations}

\levelThreeTitle{Back to the selection sort}

\levelThreeTitle{Exercises}
